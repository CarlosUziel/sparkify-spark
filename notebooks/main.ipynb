{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Data Lakes with Spark (on AWS EMR)\n",
    "This notebook creates a data lake from data hosted in S3 and executes the necessary ETL instructions using Spark in an AWS EMR cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "from pyspark.sql import DataFrame\n",
    "from IPython.core import display as ICD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path: str = \"../src\"\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl import main as run_etl\n",
    "from utils import process_config, create_spark_session\n",
    "from table_schemas import TABLES_SCHEMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_config, dl_config = (\n",
    "    process_config(Path(os.getcwd()).parent.joinpath(\"_user.cfg\")),\n",
    "    process_config(Path(os.getcwd()).parent.joinpath(\"dl.cfg\")),\n",
    ")\n",
    "spark = create_spark_session(user_config, dl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract, Transform and Load data (ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform example queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Get first 5 rows of every table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_prefix = f\"s3a://{dl_config.get('S3', 'DEST_BUCKET_NAME')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work well for partitioned tables, needs to be fixed\n",
    "for table_name, table_schema in TABLES_SCHEMAS.items():\n",
    "    table_df = spark.read.parquet(f\"{bucket_prefix}/{table_name}\", schema=table_schema)\n",
    "    n_elem = table_df.count()\n",
    "    table_df_preview = spark.createDataFrame(\n",
    "        table_df.take(5), schema=table_schema\n",
    "    ).toPandas()\n",
    "\n",
    "    print(f\"First 5 rows of {table_name}:\")\n",
    "    ICD.display(table_df_preview)\n",
    "    print(f\"The full table contains a total of {n_elem} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Who are the top 5 users with the highest activity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_df = spark.read.parquet(\n",
    "    f\"{bucket_prefix}/songplays\", schema=TABLES_SCHEMAS[\"songplays\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    songplays_df.groupBy(\"user_id\").count().sort(\"count\", ascending=False).take(5),\n",
    "    schema=table_schema,\n",
    ").toPandas()\n",
    "ICD.display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('sparkify_spark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b647ad688b86c98a24e7df79ac411f55c05c07be956fd007a2b8d7b00335a6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
